{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data\n",
    "First we have to get all the needed data. We will use web3py with Pancake bet contract (0x0E3A8078EDD2021dadcdE733C6b4a86E51EE8f07) and requests with bscscan API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add needed modules folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "if module_path + '/src' not in sys.path:\n",
    "    sys.path.append(module_path + '/src')\n",
    "\n",
    "# os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting active players\r\n",
    "Use get_active_players.py main() with start and end time. It may be required to change the main function loop to iterate through more past transactions.\r\n",
    "Also change CURRENT_BLOCK_NUMBER to the current one - to start checking transactions before this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import get_active_players\n",
    "\n",
    "# The same timeframe as for the training set\n",
    "time_from_active_players = int(datetime.datetime(2022, 10, 28, 0, 0).timestamp())\n",
    "time_to_active_players = int(datetime.datetime(2022, 12, 28, 0, 0).timestamp())\n",
    "active_players_file = f'../data/active_players_2months.csv'\n",
    "\n",
    "get_active_players.make_active_players_file(time_from_active_players, time_to_active_players, 70, active_players_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of active players and their occurence within given timeframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "active_players_df = pd.read_csv(active_players_file, index_col=False)\n",
    "\n",
    "active_players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only players that have made more than 100 bets in the selected timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of players to download data of (you can get them from the \"Getting active players\" section or from the Pancake Prediction leaderboard)\n",
    "main_wallets_list = active_players_df.loc[active_players_df['count'] > 100, 'player'].tolist()\n",
    "print(f\"Selected {len(main_wallets_list)} wallets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe to do all the computing (Training + Validation + Test sets)\n",
    "time_from_get_bets = int(datetime.datetime(2022, 10, 28, 0, 0).timestamp())\n",
    "time_to_get_bets = int(datetime.datetime(2023, 1, 28, 0, 0).timestamp())\n",
    "\n",
    "  # to get all the transactions from bscscan api (depends on the time_from_get_bets)\n",
    "block_from_get_bets = 22394098\n",
    "block_to_get_bets = 25624791\n",
    "\n",
    "# Downloading players bet history settings\n",
    "players_data_folder = '../data/players_data/'\n",
    "\n",
    "# Download rounds settings (check the rounds range according to the time_from_get_bets and time_to_get_bets values in the blockchain)\n",
    "from_round = 36000\n",
    "to_round = 65000\n",
    "rounds_file = '../data/rounds_data/final_rounds_data.csv'\n",
    "\n",
    "# Final data\n",
    "final_data_folder = '../data/merged_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading players bet history\r\n",
    "We have to collect every needed player transaction data and save to separate JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_players_bets\n",
    "print(len(main_wallets_list))\n",
    "download_players_bets.main_concurrent(main_wallets_list, players_data_folder, block_from_get_bets, block_to_get_bets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading rounds history\n",
    "Use web3py to download all the rounds info and save it to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_rounds\n",
    "downloader = download_rounds.RoundsDownloader()\n",
    "\n",
    "data = downloader.download_rounds(from_round, to_round)  # Download rounds FROM-TO, we only need the last few months\n",
    "downloader.save_rounds(rounds_file, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "Before using all the data we have to create dataframes in CSV files with all the needed stuff in one place\n",
    "The data will be stored in 2 saparate CSV files - one containing the info about the bet type of each analyzed player, the other one about the bet size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze_players\n",
    "\n",
    "rounds_df = analyze_players.load_rounds_data(rounds_file)\n",
    "    \n",
    "analyze_players.create_final_csv_files(players_data_folder, final_data_folder, rounds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_bet_amount.csv file contains data about all the rounds and separate columns for each players bet amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bet_amount_df = pd.read_csv(f'{final_data_folder}final_bet_amount.csv')\n",
    "final_bet_amount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_bet_amount.csv file contains data about all the rounds and separate columns for each players bet amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_player_bet_df = pd.read_csv(f'{final_data_folder}final_player_bet.csv', low_memory=False)\n",
    "final_player_bet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the best players\n",
    "We have collected the data about the active players in the given dataframe, but not all players are worth following. Let's check what players have high win ratio or made the most money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the timeframe for training&validation&test sets\r\n",
    "We have downloaded the data in timeframe:\r\n",
    "2022.10.28 - 2023.01.28\r\n",
    "\r\n",
    "Let's make the first 2 months to be the training set, 1/2 month validation set and 1/2 month test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "time_from_training = int(datetime.datetime(2022, 10, 28, 0, 0).timestamp())\n",
    "time_to_training = int(datetime.datetime(2022, 12, 28, 0, 0).timestamp())\n",
    "\n",
    "time_from_validation = int(datetime.datetime(2022, 12, 28, 0, 0).timestamp())\n",
    "time_to_validation = int(datetime.datetime(2023, 1, 13, 0, 0).timestamp())\n",
    "\n",
    "time_from_test = int(datetime.datetime(2023, 1, 13, 0, 0).timestamp())\n",
    "time_to_test = int(datetime.datetime(2023, 1, 28, 0, 0).timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players metrics\n",
    "Let's check how many bets were actually good for each player (win ratio) and how many CAKE tokens each player earned in total and per bet. To not affect the test set, consider only data in the training set timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_players_results\n",
    "import utils\n",
    "\n",
    "player_bet_df, bet_amount_df = utils.load_players_data(time_from_training, time_to_training)\n",
    "\n",
    "players_metrics_df = check_players_results.get_players_metrics(player_bet_df, bet_amount_df)\n",
    "players_metrics_df.sort_values(by='total_profit', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_metrics_df[['total_profit', 'profit_per_bet']].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models\n",
    "\n",
    "Now we can use all the collected data for simulation purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the environment\n",
    "Since placing a bet affects the game environment (when you add money to the pool, the payout multipliers change) we have to use a simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copytrading each player\n",
    "Copytrading one player at a time, works similar to just calculating his overall profit - but if we place a bet we make changes to the environment and change the pool. So if we place the same bet as player X, both of us will make less money (assuming we win) than if he played alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulator\n",
    "player_bet_df, bet_amount_df = utils.load_players_data(time_from_training, time_to_training)\n",
    "\n",
    "# Get all wallets from player_bet_df\n",
    "wallets = player_bet_df.columns.to_list()\n",
    "not_players_list = ['epoch', 'start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price',\n",
    "                    'total_amount', 'bull_amount', 'bear_amount', 'position']\n",
    "wallets = [wallet for wallet in wallets if wallet not in not_players_list]\n",
    "\n",
    "data = []\n",
    "for wallet in wallets:\n",
    "    trades_data = simulator.copy_trade_player(player_bet_df, bet_amount_df, wallet)\n",
    "    data.append({'wallet': wallet, 'profit': trades_data['profit'].sum()})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.sort_values(by='profit', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram with the profit results, we can see the curve is Gaussian-like\n",
    "df['profit'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data to be used in the ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data\r\n",
    "For learning process we will use the data from the 2 previous CSV files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_player_bet_df, train_bet_amount_df = utils.load_players_data(time_from_training, time_to_training)\n",
    "train_player_bet_df = train_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "train_df = train_player_bet_df.merge(train_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_player_bet_df, validate_bet_amount_df = utils.load_players_data(time_from_validation, time_to_validation)\n",
    "validate_player_bet_df = validate_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "validate_df = validate_player_bet_df.merge(validate_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "validate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_player_bet_df, test_bet_amount_df = utils.load_players_data(time_from_test, time_to_test)\n",
    "test_player_bet_df = test_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "test_df = test_player_bet_df.merge(test_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to be well balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data before the training\r\n",
    "1. Because NaN values provide no information we can delete all the rows that contain only NaNs. This would work in the production environment as if we get no information about the current round we can simply pass that round (so we don't place any bet).\r\n",
    "2. For the simplicity we will remove all the rows with 'position' == 'House' so we end up with a binary classification problem. Later, when finally testing the model we will add this category once again (when its House you lose no matter if you place a bet for Bull or Bear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the feature sets\r\n",
    "We will check few types of features sets to check which one provides the most useful information about the label.\r\n",
    "- total pool sizes for Bear/Bull bets (no additional info about the player: 2 columns in total (these features will be added to ALL feture sets below)s)\r\n",
    "- all 247 players bet info (Bull/Bear): 247 columns in total\r\n",
    "- all 247 players bet info (Bull/Bear) + their bet size info (in CAKE tokens): 247*2 = 494 columns in total\r\n",
    "- the best 10% of players bet info (Bull/Bear) according to their overall profit\r\n",
    "- the best 10% of players bet info (Bull/Bear) according to their overall profit + their bet size info (in CAKE tokens)\r\n",
    "- the best 20% of players bet info (Bull/Bear) according to their overall profit\r\n",
    "- the best 20% of players bet info (Bull/Bear) according to their overall profit + their bet size info (in CAKE tokens)\r\n",
    "- total amount of CAKE tokens bets for Bull/Bear of the best 10% of players + total number of bets (Bull/Bear) of the best 10% of players: 4 columns in total\r\n",
    "- total amount of CAKE tokens bets for Bull/Bear of the best 20% of players + total number of bets (Bull/Bear) of the best 20% of players: 4 columns in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of the best players (by total profit)\n",
    "# There are 247 wallets in total, lets check how many is 10% and 20%\n",
    "p10 = int(len(players_metrics_df)*0.1)\n",
    "p20 = int(len(players_metrics_df)*0.2)\n",
    "print(p10, p20)\n",
    "\n",
    "best_10p_players_list = players_metrics_df.sort_values(by='total_profit', ascending=False).reset_index(drop=True).loc[:p10-1, 'player'].values.tolist()\n",
    "best_20p_players_list = players_metrics_df.sort_values(by='total_profit', ascending=False).reset_index(drop=True).loc[:p20-1, 'player'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the needed columns names in the lists\n",
    "all_players_bet_cols = [col+'_bet' for col in main_wallets_list]\n",
    "all_players_amount_cols = [col+'_amount' for col in main_wallets_list]\n",
    "\n",
    "best_10p_players_bet_cols = [col+'_bet' for col in best_10p_players_list]\n",
    "best_10p_players_amount_cols = [col+'_amount' for col in best_10p_players_list]\n",
    "\n",
    "best_20p_players_bet_cols = [col+'_bet' for col in best_20p_players_list]\n",
    "best_20p_players_amount_cols = [col+'_amount' for col in best_20p_players_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, cols_to_leave_arg):\n",
    "    cols_to_leave = cols_to_leave_arg.copy()  # Make a copy of the list to not remove the elements from the original list\n",
    "\n",
    "    df = df[df[\"position\"] != \"House\"]  # To make the problem binary classification\n",
    "    df = df[df[\"total_amount\"] != 0]  # Do not consider the rounds where no one bet\n",
    "    df = df[cols_to_leave]\n",
    "\n",
    "    # Remove nan rows (where none from selected players placed any bet)\n",
    "    not_nan_cols = ['position', 'bull_amount', 'bear_amount']\n",
    "    for element in not_nan_cols:\n",
    "        cols_to_leave.remove(element)\n",
    "\n",
    "    if len(cols_to_leave) > 0:\n",
    "        df = df.dropna(subset=cols_to_leave, how='all')\n",
    "\n",
    "    # Get the list of the categorical cols and do one hot encoding\n",
    "    categorical_cols = df.select_dtypes(include=['category']).columns.to_list()\n",
    "    categorical_cols.remove('position')\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        one_hot_encoded_cols = pd.get_dummies(df[col], prefix_sep='_', prefix=col)\n",
    "\n",
    "        # Drop column with 'House' in the name (as the player can only bet on Bull or Bear)\n",
    "        one_hot_encoded_cols = one_hot_encoded_cols.drop([col + '_House'], axis=1)\n",
    "        df = df.drop([col], axis=1).join(one_hot_encoded_cols)\n",
    "\n",
    "    df = df.replace(\"Bull\", 1)\n",
    "    df = df.replace(\"Bear\", 0)\n",
    "\n",
    "    df = df.replace(\"House\", np.nan)  # Although the House rows are dropped, we have to replace it to remove the category\n",
    "    df = df.astype(float)\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set1 = ['bull_amount', 'bear_amount', 'position']\n",
    "feature_set1_train_df = prepare_data(train_df, feature_set1)\n",
    "feature_set1_validate_df = prepare_data(validate_df, feature_set1)\n",
    "feature_set1_test_df = prepare_data(train_df, feature_set1)\n",
    "\n",
    "feature_set2 = all_players_bet_cols.copy() # Keep the features from the feature_set_1 as they have predicting power\n",
    "feature_set2.extend(feature_set1)\n",
    "feature_set2_train_df = prepare_data(train_df, feature_set2)\n",
    "feature_set2_validate_df = prepare_data(validate_df, feature_set2)\n",
    "feature_set2_test_df = prepare_data(train_df, feature_set2)\n",
    "\n",
    "feature_set3 = all_players_bet_cols.copy() + all_players_amount_cols.copy()\n",
    "feature_set3.extend(feature_set1)\n",
    "feature_set3_train_df = prepare_data(train_df, feature_set3)\n",
    "feature_set3_validate_df = prepare_data(validate_df, feature_set3)\n",
    "feature_set3_test_df = prepare_data(train_df, feature_set3)\n",
    "\n",
    "feature_set4 = best_10p_players_bet_cols.copy()\n",
    "feature_set4.extend(feature_set1)\n",
    "feature_set4_train_df = prepare_data(train_df, feature_set4)\n",
    "feature_set4_validate_df = prepare_data(validate_df, feature_set4)\n",
    "feature_set4_test_df = prepare_data(train_df, feature_set4)\n",
    "\n",
    "feature_set5 = best_10p_players_bet_cols.copy() + best_10p_players_amount_cols.copy()\n",
    "feature_set5.extend(feature_set1)\n",
    "feature_set5_train_df = prepare_data(train_df, feature_set5)\n",
    "feature_set5_validate_df = prepare_data(validate_df, feature_set5)\n",
    "feature_set5_test_df = prepare_data(train_df, feature_set5)\n",
    "\n",
    "feature_set6 = best_20p_players_bet_cols.copy()\n",
    "feature_set6.extend(feature_set1)\n",
    "feature_set6_train_df = prepare_data(train_df, feature_set6)\n",
    "feature_set6_validate_df = prepare_data(validate_df, feature_set6)\n",
    "feature_set6_test_df = prepare_data(train_df, feature_set6)\n",
    "\n",
    "feature_set7 = best_20p_players_bet_cols.copy() + best_20p_players_amount_cols.copy()\n",
    "feature_set7.extend(feature_set1)\n",
    "feature_set7_train_df = prepare_data(train_df, feature_set7)\n",
    "feature_set7_validate_df = prepare_data(validate_df, feature_set7)\n",
    "feature_set7_test_df = prepare_data(train_df, feature_set7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_df(df, bet_type_columns, bet_size_columns, additional_columns):\n",
    "    filtered_df = df[bet_size_columns + bet_type_columns + ['epoch', 'total_amount'] + additional_columns].copy()\n",
    "\n",
    "    # Calculate Bear number for each row\n",
    "    filtered_df['best_bear_number'] = filtered_df[bet_type_columns].isin(['Bear']).sum(axis=1)\n",
    "    filtered_df['best_bull_number'] = filtered_df[bet_type_columns].isin(['Bull']).sum(axis=1)\n",
    "\n",
    "    filtered_df['best_bear_amount'] = 0\n",
    "    filtered_df['best_bull_amount'] = 0\n",
    "\n",
    "    # Replace NaN with 0 in the bet size columns\n",
    "    filtered_df[bet_size_columns] = filtered_df[bet_size_columns].fillna(0)\n",
    "\n",
    "    # Iterate through each wallet's columns to aggregate the data\n",
    "    for bet_type_column in bet_type_columns:\n",
    "        bet_size_column = bet_type_column.replace('_bet', '_amount')\n",
    "\n",
    "        wallet_df = filtered_df[['epoch', bet_size_column, bet_type_column]]\n",
    "\n",
    "        bear_bets = np.where(wallet_df[bet_type_column] == 'Bear', wallet_df[bet_size_column], 0)\n",
    "        bull_bets = np.where(wallet_df[bet_type_column] == 'Bull', wallet_df[bet_size_column], 0)\n",
    "\n",
    "        filtered_df['best_bear_amount'] += bear_bets\n",
    "        filtered_df['best_bull_amount'] += bull_bets\n",
    "\n",
    "    # Drop rows where all columns are 0\n",
    "    filtered_df = filtered_df[(filtered_df[bet_size_columns] != 0).any(axis=1)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need separate function for these 2\n",
    "feature_set8 = ['best_bear_number', 'best_bull_number', 'best_bear_amount', 'best_bull_amount']\n",
    "feature_set8.extend(feature_set1)\n",
    "feature_set8_train_df = create_aggregated_df(train_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "feature_set8_validate_df = create_aggregated_df(validate_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "feature_set8_test_df = create_aggregated_df(test_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "\n",
    "feature_set8_train_df = prepare_data(feature_set8_train_df, feature_set8)\n",
    "feature_set8_validate_df = prepare_data(feature_set8_validate_df, feature_set8)\n",
    "feature_set8_test_df = prepare_data(feature_set8_test_df, feature_set8)\n",
    "\n",
    "feature_set9 = feature_set8\n",
    "feature_set9_train_df = create_aggregated_df(train_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "feature_set9_validate_df = create_aggregated_df(validate_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "feature_set9_test_df = create_aggregated_df(test_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "\n",
    "feature_set9_train_df = prepare_data(feature_set9_train_df, feature_set9)\n",
    "feature_set9_validate_df = prepare_data(feature_set9_validate_df, feature_set9)\n",
    "feature_set9_test_df = prepare_data(feature_set9_test_df, feature_set9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the feature sets info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_sets_dfs_list = [feature_set1_train_df, feature_set2_train_df, feature_set3_train_df, feature_set4_train_df, feature_set5_train_df, feature_set6_train_df, feature_set7_train_df, feature_set8_train_df, feature_set9_train_df]\n",
    "\n",
    "for i, feature_set_df in enumerate(feature_sets_dfs_list):\n",
    "    print(f\"--- Feature set {i+1} Total: {feature_set_df.shape[1]} ---\")\n",
    "    print(feature_set_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the outliers\n",
    "All possible outliers represent natural variations and they will be left in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "The columns containing info about each players bet type (Bull/Bear or None, so categorical data) have been One-Hot encoded. The rest of the columns containing info about each players bet size (so numerical data) need to be scaled. Let's check the distribution of some random columns to choose the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(feature_set1_train_df.columns)\n",
    "feature_set1_train_df.iloc[:, 0].hist(range=(0,100), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(feature_set3_train_df.columns)\n",
    "feature_set3_train_df.iloc[:, 15].hist(range=(0,5), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set5_train_df.columns)\n",
    "feature_set5_train_df.iloc[:, 25].hist(range=(0,100), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set7_train_df.columns)\n",
    "feature_set7_train_df.iloc[:, 32].hist(range=(0,15), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set9_train_df.columns)\n",
    "feature_set9_train_df.iloc[:, 0].hist(range=(0,15), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set9_train_df.iloc[:, 2].hist(range=(0,15), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data distribution seems to not be normal, so we will use the MinMaxScaler from the scikit-learn library. The scaler have to be fit on the training data, then used to scale the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_amount_columns(train_df, validate_df, test_df):\n",
    "    # Train MinMaxScalers for each training dataset\n",
    "    scaler = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    # Basically we want to scale all the columns that contain 'amount' substring\n",
    "    columns_to_scale = [col for col in train_df.columns if 'amount' in col]\n",
    "\n",
    "    # Fit the scalers on each feature set training data\n",
    "    train_df[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale])\n",
    "    validate_df[columns_to_scale] = scaler.transform(validate_df[columns_to_scale])\n",
    "    test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n",
    "\n",
    "    return train_df, validate_df, test_df\n",
    "\n",
    "feature_set1_train_df, feature_set1_validate_df, feature_set1_test_df = scale_amount_columns(feature_set1_train_df, feature_set1_validate_df, feature_set1_test_df)\n",
    "feature_set2_train_df, feature_set2_validate_df, feature_set2_test_df = scale_amount_columns(feature_set2_train_df, feature_set2_validate_df, feature_set2_test_df)\n",
    "feature_set3_train_df, feature_set3_validate_df, feature_set3_test_df = scale_amount_columns(feature_set3_train_df, feature_set3_validate_df, feature_set3_test_df)\n",
    "feature_set4_train_df, feature_set4_validate_df, feature_set4_test_df = scale_amount_columns(feature_set4_train_df, feature_set4_validate_df, feature_set4_test_df)\n",
    "feature_set5_train_df, feature_set5_validate_df, feature_set5_test_df = scale_amount_columns(feature_set5_train_df, feature_set5_validate_df, feature_set5_test_df)\n",
    "feature_set6_train_df, feature_set6_validate_df, feature_set6_test_df = scale_amount_columns(feature_set6_train_df, feature_set6_validate_df, feature_set6_test_df)\n",
    "feature_set7_train_df, feature_set7_validate_df, feature_set7_test_df = scale_amount_columns(feature_set7_train_df, feature_set7_validate_df, feature_set7_test_df)\n",
    "feature_set8_train_df, feature_set8_validate_df, feature_set8_test_df = scale_amount_columns(feature_set8_train_df, feature_set8_validate_df, feature_set8_test_df)\n",
    "feature_set9_train_df, feature_set9_validate_df, feature_set9_test_df = scale_amount_columns(feature_set9_train_df, feature_set9_validate_df, feature_set9_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set2_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold\n",
    "As after removing the rounds with 'House' position we end up with binary classification problem, we want to leave it like that. But in production environment we can bet Bull/Bear or pass the round without betting and wait for for the model to give confident prediction.\n",
    "We can get the probability for each class with sklearn function predict_proba. 0.5-0.5 means the model is completely not sure which class to choose, while 0.9-0.1 means its high probability that the record will be classified as the first class.\n",
    "Let's add another hyperparameter - threshold. The model will return a class label only when the probability will be higher than the threshold. Threshold can be in range of 0.5-1, however we have to determine it experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the scikit-learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def run(x_train, y_train, x_test, y_test, clf, threshold=0.7):\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    total_accuracy = clf.score(x_test, y_test)\n",
    "\n",
    "    predicted_probability = clf.predict_proba(x_test)\n",
    "\n",
    "    # Create a df to compare the predicted_probability with the real y_test\n",
    "    df = pd.DataFrame(predicted_probability, columns=[\"Bear\", \"Bull\"])\n",
    "    df[\"position\"] = y_test.values\n",
    "    df[\"predicted_position\"] = np.where(df[\"Bear\"] > threshold, 0, np.nan)\n",
    "    df[\"predicted_position\"] = np.where(df[\"Bull\"] > threshold, 1, df[\"predicted_position\"])\n",
    "\n",
    "    df[\"correct\"] = np.where(df[\"position\"] == df[\"predicted_position\"], 1, 0)\n",
    "\n",
    "    # Get accuracy when probability is above threshold\n",
    "    accuracy = df[\"correct\"].sum() / (df['predicted_position'].count() + 0.0000001)  # Count only not NaN values\n",
    "\n",
    "    return {'classifier': str(clf), 'threshold': threshold, 'total_accuracy': total_accuracy,\n",
    "            'total_number_of_bets': len(y_test), 'accuracy_above_threshold': accuracy,\n",
    "            'number_of_bets_above_threshold': df['predicted_position'].count()}\n",
    "\n",
    "\n",
    "def main(train_df, validate_df, clfs, threshold_list, cols_to_leave):\n",
    "\n",
    "    train_df = prepare_data(train_df, cols_to_leave)\n",
    "    validate_df = prepare_data(validate_df, cols_to_leave)\n",
    "\n",
    "    # X_train, y_train = train_df.drop([\"position\"], axis=1), train_df[\"position\"]\n",
    "    X_test, y_test = validate_df.drop([\"position\"], axis=1), validate_df[\"position\"]\n",
    "    X_train, y_train = train_df.drop([\"position\"], axis=1), train_df[\"position\"]\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for clf in clfs:\n",
    "        for threshold in threshold_list:\n",
    "            data = run(X_train, y_train, X_test, y_test, clf, threshold)\n",
    "            data_list.append(data)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "clfs = [RandomForestClassifier(100), RandomForestClassifier(500), RandomForestClassifier(1000),\n",
    "        DecisionTreeClassifier(criterion='entropy'), MLPClassifier(alpha=1, max_iter=1000), GaussianNB(),\n",
    "        KNeighborsClassifier(3), KNeighborsClassifier(5), KNeighborsClassifier(7),\n",
    "        KNeighborsClassifier(9), KNeighborsClassifier(11), LogisticRegression(max_iter=1000)]\n",
    "\n",
    "svc_clfs = [SVC(kernel='rbf', probability=True), SVC(kernel='linear', probability=True),\n",
    "            SVC(kernel='rbf', probability=True, C=10), SVC(kernel='linear', probability=True, C=10),\n",
    "            SVC(kernel='rbf', probability=True, C=50), SVC(kernel='linear', probability=True, C=50)]\n",
    "\n",
    "threshold_list = [0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #1: total pool sizes for Bear/Bull bets (no additional info about the players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment1_df = main(train_df=train_df, validate_df=validate_df, clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2_df = main(train_df=train_df, validate_df=validate_df, clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #2: all 247 players bet info (Bull/Bear): 247 columns in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment3_df = main(train_df=train_df, validate_df=validate_df, clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This features set model takes much longer to train\n",
    "experiment3_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
