{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Pancake Prediction?\n",
    "Pancake Prediction is a feature within PancakeSwap, a popular decentralized finance (DeFi) platform built on the Binance Smart Chain. This feature allows users to predict whether the price of certain cryptocurrencies, specifically CAKE and BNB, will rise or fall within a specified time frame, usually every five minutes.\n",
    "\n",
    "Users participate in Pancake Prediction v2 by selecting either an 'Enter UP' option if they believe the price will rise, or an 'Enter DOWN' option if they think it will fall. To make a prediction, users commit a certain amount of BNB or CAKE tokens. If their prediction is correct, they win a reward, which is deposited into their digital wallee\n",
    "\n",
    "\n",
    "It's important to note that PancakeSwap, including its prediction feature, operates as a decentralized exchange, meaning it doesn't have a central authority overseeing it. The platform earns revenue by charging a small trading fee on each transaction, which is then split between liquidity providers and the PancakeSwap t\n",
    "\n",
    "Pancake Prediction v2 on PancakeSwap shares similarities with binary options trading. Both involve predicting the direction of an asset's price movement within a specified time frame (5 minutes). If your prediction is correct, you win a reward; if not, you lose your investment.d: Both are high-risk and high-reward activities. The simplicity of the decision (up or down) masks the difficulty of making accurate predictions, especially in short time frame\n",
    "\n",
    "Link: https://pancakeswap.finance/prediction?token=CAKE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data\n",
    "First we have to get all the needed data. We will use web3py with Pancake bet contract (0x0E3A8078EDD2021dadcdE733C6b4a86E51EE8f07) and requests with bscscan API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add needed modules folders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "if module_path + '/src' not in sys.path:\n",
    "    sys.path.append(module_path + '/src')\n",
    "\n",
    "# os.chdir(\"..\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies from the requirements.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting active players\r\n",
    "Use get_active_players.py main() with start and end time. It may be required to change the main function loop to iterate through more past transactions.\r\n",
    "Also change CURRENT_BLOCK_NUMBER to the current one - to start checking transactions before this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import get_active_players\n",
    "\n",
    "# The same timeframe as for the training set\n",
    "time_from_active_players = int(datetime.datetime(2022, 10, 28, 0, 0).timestamp())\n",
    "time_to_active_players = int(datetime.datetime(2022, 12, 28, 0, 0).timestamp())\n",
    "active_players_file = f'../data/active_players_2months.csv'\n",
    "\n",
    "get_active_players.make_active_players_file(time_from_active_players, time_to_active_players, 70, active_players_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of active players and their occurence within given timeframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "active_players_df = pd.read_csv(active_players_file, index_col=False)\n",
    "\n",
    "active_players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only players that have made more than 100 bets in the selected timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of players to download data of (you can get them from the \"Getting active players\" section or from the Pancake Prediction leaderboard)\n",
    "main_wallets_list = active_players_df.loc[active_players_df['count'] > 100, 'player'].tolist()\n",
    "print(f\"Selected {len(main_wallets_list)} wallets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeframe to do all the computing (Training + Validation + Test sets)\n",
    "time_from_get_bets = int(datetime.datetime(2022, 10, 28, 0, 0).timestamp())\n",
    "time_to_get_bets = int(datetime.datetime(2023, 1, 28, 0, 0).timestamp())\n",
    "\n",
    "  # to get all the transactions from bscscan api (depends on the time_from_get_bets)\n",
    "block_from_get_bets = 22394098\n",
    "block_to_get_bets = 25624791\n",
    "\n",
    "# Downloading players bet history settings\n",
    "players_data_folder = '../data/players_data/'\n",
    "\n",
    "# Download rounds settings (check the rounds range according to the time_from_get_bets and time_to_get_bets values in the blockchain)\n",
    "from_round = 36000\n",
    "to_round = 65000\n",
    "rounds_file = '../data/rounds_data/final_rounds_data.csv'\n",
    "\n",
    "# Final data\n",
    "final_data_folder = '../data/merged_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading players bet history\r\n",
    "We have to collect every needed player transaction data and save to separate JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_players_bets\n",
    "print(len(main_wallets_list))\n",
    "download_players_bets.main_concurrent(main_wallets_list, players_data_folder, block_from_get_bets, block_to_get_bets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading rounds history\n",
    "Use web3py to download all the rounds info and save it to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import download_rounds\n",
    "downloader = download_rounds.RoundsDownloader()\n",
    "\n",
    "data = downloader.download_rounds(from_round, to_round)  # Download rounds FROM-TO, we only need the last few months\n",
    "downloader.save_rounds(rounds_file, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "Before using all the data we have to create dataframes in CSV files with all the needed stuff in one place\n",
    "The data will be stored in 2 saparate CSV files - one containing the info about the bet type of each analyzed player, the other one about the bet size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analyze_players\n",
    "\n",
    "rounds_df = analyze_players.load_rounds_data(rounds_file)\n",
    "    \n",
    "analyze_players.create_final_csv_files(players_data_folder, final_data_folder, rounds_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_bet_amount.csv file contains data about all the rounds and separate columns for each players bet amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_bet_amount_df = pd.read_csv(f'{final_data_folder}final_bet_amount.csv')\n",
    "final_bet_amount_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_bet_amount.csv file contains data about all the rounds and separate columns for each players bet amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_player_bet_df = pd.read_csv(f'{final_data_folder}final_player_bet.csv', low_memory=False)\n",
    "final_player_bet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the best players\n",
    "We have collected the data about the active players in the given dataframe, but not all players are worth following. Let's check what players have high win ratio or made the most money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the timeframe for training&validation&test sets\r\n",
    "We have downloaded the data in timeframe:\r\n",
    "2022.10.28 - 2023.01.28\r\n",
    "\r\n",
    "Let's make the first 2 months to be the training set, 1/2 month validation set and 1/2 month test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "time_from_training = int(datetime.datetime(2022, 10, 28, 0, 0).timestamp())\n",
    "time_to_training = int(datetime.datetime(2022, 12, 28, 0, 0).timestamp())\n",
    "\n",
    "time_from_validation = int(datetime.datetime(2022, 12, 28, 0, 0).timestamp())\n",
    "time_to_validation = int(datetime.datetime(2023, 1, 13, 0, 0).timestamp())\n",
    "\n",
    "time_from_test = int(datetime.datetime(2023, 1, 13, 0, 0).timestamp())\n",
    "time_to_test = int(datetime.datetime(2023, 1, 28, 0, 0).timestamp())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Players metrics\n",
    "Let's check how many bets were actually good for each player (win ratio) and how many CAKE tokens each player earned in total and per bet. To not affect the test set, consider only data in the training set timeframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_players_results\n",
    "import utils\n",
    "\n",
    "player_bet_df, bet_amount_df = utils.load_players_data(time_from_training, time_to_training)\n",
    "\n",
    "players_metrics_df = check_players_results.get_players_metrics(player_bet_df, bet_amount_df)\n",
    "players_metrics_df.sort_values(by='total_profit', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_metrics_df[['total_profit', 'profit_per_bet']].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models\n",
    "\n",
    "Now we can use all the collected data for simulation purposes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating the environment\n",
    "Since placing a bet affects the game environment (when you add money to the pool, the payout multipliers change) we have to use a simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copytrading each player\n",
    "Copytrading one player at a time, works similar to just calculating his overall profit - but if we place a bet we make changes to the environment and change the pool. So if we place the same bet as player X, both of us will make less money (assuming we win) than if he played alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulator\n",
    "player_bet_df, bet_amount_df = utils.load_players_data(time_from_training, time_to_training)\n",
    "\n",
    "# Get all wallets from player_bet_df\n",
    "wallets = player_bet_df.columns.to_list()\n",
    "not_players_list = ['epoch', 'start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price',\n",
    "                    'total_amount', 'bull_amount', 'bear_amount', 'position']\n",
    "wallets = [wallet for wallet in wallets if wallet not in not_players_list]\n",
    "\n",
    "data = []\n",
    "for wallet in wallets:\n",
    "    trades_data = simulator.copy_trade_player(player_bet_df, bet_amount_df, wallet)\n",
    "    data.append({'wallet': wallet, 'profit': trades_data['profit'].sum()})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.sort_values(by='profit', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram with the profit results, we can see the curve is Gaussian-like\n",
    "df['profit'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation hyperparameters\n",
    "We can bet different amount of money on each round, affecting the pool sizes and finally our and other players outcome. Not all rounds are worth betting - when we bet for Bull and there are 100$\\$$ in total in the Bull pool and 5$\\$$ in the Bear pool our potential profits are small but still we can loose 100% of the money invested. Thats why we should consider 2 additional hyperparameters: size of our bet (for simplicity it will be a fixed value in CAKE tokens, but in the future it could be calculated with soem formula) and minimal potential profit (as multiplier) that is needed to place a bet. These parameters will be tuned after choosing the most accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bet_size=1\n",
    "min_multiplier=1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data to be used in the ml"
   ]
  },
  {
   "attachments": {
    "242ea610-e291-44a7-a8b0-bb8ef0da184d.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the data\r\n",
    "For learning process we will use the data from the 2 previous CSV files.\n",
    "The data will be divided into 3 sets, based on the datetime (as its a timeseries data): the first method will be used with Pytorch model, the second one with scikitlearn, as we will need to tune the hyperparameters with k-fold cross-validation.\n",
    "![datasets.drawio.png](attachment:242ea610-e291-44a7-a8b0-bb8ef0da184d.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_player_bet_df, train_bet_amount_df = utils.load_players_data(time_from_training, time_to_training)\n",
    "train_player_bet_df = train_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "train_df = train_player_bet_df.merge(train_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "train_df = train_df.set_index('epoch')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_player_bet_df, validate_bet_amount_df = utils.load_players_data(time_from_validation, time_to_validation)\n",
    "validate_player_bet_df = validate_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "validate_df = validate_player_bet_df.merge(validate_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "validate_df = validate_df.set_index('epoch')\n",
    "validate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_player_bet_df, test_bet_amount_df = utils.load_players_data(time_from_test, time_to_test)\n",
    "test_player_bet_df = test_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "test_df = test_player_bet_df.merge(test_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "test_df = test_df.set_index('epoch')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset seems to be well balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will use k-fold cross-validation for the hyperparameter tuning of the sklearn models, we can merge the train and validation datasets. The final performance of the model will be checked on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_player_bet_df, merged_train_bet_amount_df = utils.load_players_data(time_from_training, time_to_validation)\n",
    "merged_train_player_bet_df = merged_train_player_bet_df.drop(columns=['start_timestamp', 'lock_timestamp', 'close_timestamp', 'lock_price', 'close_price', 'total_amount', 'bull_amount', 'bear_amount', 'position'])\n",
    "merged_train_df = merged_train_player_bet_df.merge(merged_train_bet_amount_df, on = 'epoch', suffixes=['_bet','_amount'])\n",
    "merged_train_df = merged_train_df.set_index('epoch')\n",
    "merged_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df['position'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data before the training\r\n",
    "1. Because NaN values provide no information we can delete all the rows that contain only NaNs. This would work in the production environment as if we get no information about the current round we can simply pass that round (so we don't place any bet).\r\n",
    "2. For the simplicity we will remove all the rows with 'position' == 'House' so we end up with a binary classification problem. Later, when finally testing the model we will add this category once again (when its House you lose no matter if you place a bet for Bull or Bea\n",
    "3. Do not consider transaction fees. Because many players are bots and they place their bet in the last few seconds of the round, in the production environment we should use higher gas price to be able to place the bet just after recording all the other players bets. The transaction cost per every bet will be 0.05 $\\$$ -0.2 $\\$$ r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the feature sets\r\n",
    "We will check few types of features sets to check which one provides the most useful information about the label.\r\n",
    "- total pool sizes for Bear/Bull bets (no additional info about the player: 2 columns in total (these features will be added to ALL feture sets below)s)\r\n",
    "- all 247 players bet info (Bull/Bear): 247 columns in total\r\n",
    "- all 247 players bet info (Bull/Bear) + their bet size info (in CAKE tokens): 247*2 = 494 columns in total\r\n",
    "- the best 10% of players bet info (Bull/Bear) according to their overall profit\r\n",
    "- the best 10% of players bet info (Bull/Bear) according to their overall profit + their bet size info (in CAKE tokens)\r\n",
    "- the best 20% of players bet info (Bull/Bear) according to their overall profit\r\n",
    "- the best 20% of players bet info (Bull/Bear) according to their overall profit + their bet size info (in CAKE tokens)\r\n",
    "- total amount of CAKE tokens bets for Bull/Bear of the best 10% of players + total number of bets (Bull/Bear) of the best 10% of players: 4 columns in total\r\n",
    "- total amount of CAKE tokens bets for Bull/Bear of the best 20% of players + total number of bets (Bull/Bear) of the best 20% of players: 4 columns in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of the best players (by total profit)\n",
    "# There are 247 wallets in total, lets check how many is 10% and 20%\n",
    "p10 = int(len(players_metrics_df)*0.1)\n",
    "p20 = int(len(players_metrics_df)*0.2)\n",
    "print(p10, p20)\n",
    "\n",
    "best_10p_players_list = players_metrics_df.sort_values(by='total_profit', ascending=False).reset_index(drop=True).loc[:p10-1, 'player'].values.tolist()\n",
    "best_20p_players_list = players_metrics_df.sort_values(by='total_profit', ascending=False).reset_index(drop=True).loc[:p20-1, 'player'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the needed columns names in the lists\n",
    "all_players_bet_cols = [col+'_bet' for col in main_wallets_list]\n",
    "all_players_amount_cols = [col+'_amount' for col in main_wallets_list]\n",
    "\n",
    "best_10p_players_bet_cols = [col+'_bet' for col in best_10p_players_list]\n",
    "best_10p_players_amount_cols = [col+'_amount' for col in best_10p_players_list]\n",
    "\n",
    "best_20p_players_bet_cols = [col+'_bet' for col in best_20p_players_list]\n",
    "best_20p_players_amount_cols = [col+'_amount' for col in best_20p_players_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, cols_to_leave_arg):\n",
    "    cols_to_leave = cols_to_leave_arg.copy()  # Make a copy of the list to not remove the elements from the original list\n",
    "\n",
    "    df = df[df[\"position\"] != \"House\"]  # To make the problem binary classification\n",
    "    df = df[df[\"total_amount\"] != 0]  # Do not consider the rounds where no one bet\n",
    "    df = df[cols_to_leave]\n",
    "\n",
    "    # Remove nan rows (where none from selected players placed any bet)\n",
    "    not_nan_cols = ['position', 'bull_amount', 'bear_amount']\n",
    "    for element in not_nan_cols:\n",
    "        cols_to_leave.remove(element)\n",
    "\n",
    "    if len(cols_to_leave) > 0:\n",
    "        df = df.dropna(subset=cols_to_leave, how='all')\n",
    "\n",
    "    # Get the list of the categorical cols and do one hot encoding\n",
    "    categorical_cols = df.select_dtypes(include=['category']).columns.to_list()\n",
    "    categorical_cols.remove('position')\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        one_hot_encoded_cols = pd.get_dummies(df[col], prefix_sep='_', prefix=col)\n",
    "\n",
    "        # Drop column with 'House' in the name (as the player can only bet on Bull or Bear)\n",
    "        one_hot_encoded_cols = one_hot_encoded_cols.drop([col + '_House'], axis=1)\n",
    "        df = df.drop([col], axis=1).join(one_hot_encoded_cols)\n",
    "\n",
    "    # Fill numeric columns with 0 (bet amount cols)\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "    df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    df = df.replace(\"Bull\", 1)\n",
    "    df = df.replace(\"Bear\", 0)\n",
    "\n",
    "    df = df.replace(\"House\", np.nan)  # Although the House rows are dropped, we have to replace it to remove the category\n",
    "    df = df.astype(float)\n",
    "\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set1 = ['bull_amount', 'bear_amount', 'position']\n",
    "feature_set1_train_df = prepare_data(train_df, feature_set1)\n",
    "feature_set1_validate_df = prepare_data(validate_df, feature_set1)\n",
    "feature_set1_test_df = prepare_data(train_df, feature_set1)\n",
    "feature_set1_merged_train_df = prepare_data(merged_train_df, feature_set1)\n",
    "\n",
    "feature_set2 = all_players_bet_cols.copy() # Keep the features from the feature_set_1 as they have predicting power\n",
    "feature_set2.extend(feature_set1)\n",
    "feature_set2_train_df = prepare_data(train_df, feature_set2)\n",
    "feature_set2_validate_df = prepare_data(validate_df, feature_set2)\n",
    "feature_set2_test_df = prepare_data(train_df, feature_set2)\n",
    "feature_set2_merged_train_df = prepare_data(merged_train_df, feature_set2)\n",
    "\n",
    "feature_set3 = all_players_bet_cols.copy() + all_players_amount_cols.copy()\n",
    "feature_set3.extend(feature_set1)\n",
    "feature_set3_train_df = prepare_data(train_df, feature_set3)\n",
    "feature_set3_validate_df = prepare_data(validate_df, feature_set3)\n",
    "feature_set3_test_df = prepare_data(train_df, feature_set3)\n",
    "feature_set3_merged_train_df = prepare_data(merged_train_df, feature_set3)\n",
    "\n",
    "feature_set4 = best_10p_players_bet_cols.copy()\n",
    "feature_set4.extend(feature_set1)\n",
    "feature_set4_train_df = prepare_data(train_df, feature_set4)\n",
    "feature_set4_validate_df = prepare_data(validate_df, feature_set4)\n",
    "feature_set4_test_df = prepare_data(train_df, feature_set4)\n",
    "feature_set4_merged_train_df = prepare_data(merged_train_df, feature_set4)\n",
    "\n",
    "feature_set5 = best_10p_players_bet_cols.copy() + best_10p_players_amount_cols.copy()\n",
    "feature_set5.extend(feature_set1)\n",
    "feature_set5_train_df = prepare_data(train_df, feature_set5)\n",
    "feature_set5_validate_df = prepare_data(validate_df, feature_set5)\n",
    "feature_set5_test_df = prepare_data(train_df, feature_set5)\n",
    "feature_set5_merged_train_df = prepare_data(merged_train_df, feature_set5)\n",
    "\n",
    "feature_set6 = best_20p_players_bet_cols.copy()\n",
    "feature_set6.extend(feature_set1)\n",
    "feature_set6_train_df = prepare_data(train_df, feature_set6)\n",
    "feature_set6_validate_df = prepare_data(validate_df, feature_set6)\n",
    "feature_set6_test_df = prepare_data(train_df, feature_set6)\n",
    "feature_set6_merged_train_df = prepare_data(merged_train_df, feature_set6)\n",
    "\n",
    "feature_set7 = best_20p_players_bet_cols.copy() + best_20p_players_amount_cols.copy()\n",
    "feature_set7.extend(feature_set1)\n",
    "feature_set7_train_df = prepare_data(train_df, feature_set7)\n",
    "feature_set7_validate_df = prepare_data(validate_df, feature_set7)\n",
    "feature_set7_test_df = prepare_data(train_df, feature_set7)\n",
    "feature_set7_merged_train_df = prepare_data(merged_train_df, feature_set7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aggregated_df(df, bet_type_columns, bet_size_columns, additional_columns):\n",
    "    filtered_df = df[bet_size_columns + bet_type_columns + ['total_amount'] + additional_columns].copy()\n",
    "\n",
    "    # Calculate Bear number for each row\n",
    "    filtered_df['best_bear_number'] = filtered_df[bet_type_columns].isin(['Bear']).sum(axis=1)\n",
    "    filtered_df['best_bull_number'] = filtered_df[bet_type_columns].isin(['Bull']).sum(axis=1)\n",
    "\n",
    "    filtered_df['best_bear_amount'] = 0\n",
    "    filtered_df['best_bull_amount'] = 0\n",
    "\n",
    "    # Replace NaN with 0 in the bet size columns\n",
    "    filtered_df[bet_size_columns] = filtered_df[bet_size_columns].fillna(0)\n",
    "\n",
    "    # Iterate through each wallet's columns to aggregate the data\n",
    "    for bet_type_column in bet_type_columns:\n",
    "        bet_size_column = bet_type_column.replace('_bet', '_amount')\n",
    "\n",
    "        wallet_df = filtered_df[[bet_size_column, bet_type_column]]\n",
    "\n",
    "        bear_bets = np.where(wallet_df[bet_type_column] == 'Bear', wallet_df[bet_size_column], 0)\n",
    "        bull_bets = np.where(wallet_df[bet_type_column] == 'Bull', wallet_df[bet_size_column], 0)\n",
    "\n",
    "        filtered_df['best_bear_amount'] += bear_bets\n",
    "        filtered_df['best_bull_amount'] += bull_bets\n",
    "\n",
    "    # Drop rows where all columns are 0\n",
    "    filtered_df = filtered_df[(filtered_df[bet_size_columns] != 0).any(axis=1)]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need separate function for these 2\n",
    "feature_set8 = ['best_bear_number', 'best_bull_number', 'best_bear_amount', 'best_bull_amount']\n",
    "feature_set8.extend(feature_set1)\n",
    "feature_set8_train_df = create_aggregated_df(train_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "feature_set8_validate_df = create_aggregated_df(validate_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "feature_set8_test_df = create_aggregated_df(test_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "feature_set8_merged_train_df = create_aggregated_df(merged_train_df, best_10p_players_bet_cols, best_10p_players_amount_cols, feature_set1.copy())\n",
    "\n",
    "feature_set8_train_df = prepare_data(feature_set8_train_df, feature_set8)\n",
    "feature_set8_validate_df = prepare_data(feature_set8_validate_df, feature_set8)\n",
    "feature_set8_test_df = prepare_data(feature_set8_test_df, feature_set8)\n",
    "feature_set8_merged_train_df = prepare_data(feature_set8_merged_train_df, feature_set8)\n",
    "\n",
    "feature_set9 = feature_set8\n",
    "feature_set9_train_df = create_aggregated_df(train_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "feature_set9_validate_df = create_aggregated_df(validate_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "feature_set9_test_df = create_aggregated_df(test_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "feature_set9_merged_train_df = create_aggregated_df(merged_train_df, best_20p_players_bet_cols, best_20p_players_amount_cols, feature_set1.copy())\n",
    "\n",
    "feature_set9_train_df = prepare_data(feature_set9_train_df, feature_set9)\n",
    "feature_set9_validate_df = prepare_data(feature_set9_validate_df, feature_set9)\n",
    "feature_set9_test_df = prepare_data(feature_set9_test_df, feature_set9)\n",
    "feature_set9_merged_train_df = prepare_data(feature_set9_merged_train_df, feature_set9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the feature sets info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_sets_dfs_list = [feature_set1_merged_train_df, feature_set2_merged_train_df, feature_set3_merged_train_df, feature_set4_merged_train_df, feature_set5_merged_train_df, feature_set6_merged_train_df, feature_set7_merged_train_df, feature_set8_merged_train_df, feature_set9_merged_train_df]\n",
    "\n",
    "for i, feature_set_df in enumerate(feature_sets_dfs_list):\n",
    "    print(f\"--- Feature set {i+1} Total: {feature_set_df.shape[1]} ---\")\n",
    "    print(feature_set_df.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing the outliers\n",
    "All possible outliers represent natural variations and they will be left in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "The columns containing info about each players bet type (Bull/Bear or None, so categorical data) have been One-Hot encoded. The rest of the columns containing info about each players bet size (so numerical data) need to be scaled. Let's check the distribution of some random columns to choose the scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(feature_set1_merged_train_df.columns)\n",
    "feature_set1_merged_train_df.iloc[:, 0].hist(range=(0,100), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(feature_set3_merged_train_df.columns)\n",
    "feature_set3_merged_train_df.iloc[:, 15].hist(range=(0,5), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set5_merged_train_df.columns)\n",
    "feature_set5_merged_train_df.iloc[:, 25].hist(range=(0,100), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set7_merged_train_df.columns)\n",
    "feature_set7_merged_train_df.iloc[:, 32].hist(range=(0,15), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_set9_merged_train_df.columns)\n",
    "feature_set9_merged_train_df.iloc[:, 0].hist(range=(0,15), bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set9_merged_train_df.iloc[:, 2].hist(range=(0,15), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data distribution seems to not be normal, so we will use the MinMaxScaler from the scikit-learn library. The scaler have to be fit on the training data, then used to scale the training, validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_amount_columns(merged_train_df, train_df, validate_df, test_df):\n",
    "    # Train MinMaxScalers for each training dataset\n",
    "    scaler = MinMaxScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    # Basically we want to scale all the columns that contain 'amount' or 'number' substring\n",
    "    substrings = ['amount', 'number']\n",
    "    columns_to_scale = [col for col in train_df.columns if any(sub in col for sub in substrings)]\n",
    "\n",
    "    # Fit the scalers on each feature set training data\n",
    "    merged_train_df[columns_to_scale] = scaler.fit_transform(merged_train_df[columns_to_scale])\n",
    "    train_df[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale])\n",
    "    validate_df[columns_to_scale] = scaler.transform(validate_df[columns_to_scale])\n",
    "    test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n",
    "\n",
    "    return merged_train_df, train_df, validate_df, test_df\n",
    "\n",
    "feature_set1_merged_train_df, feature_set1_train_df, feature_set1_validate_df, feature_set1_test_df = scale_amount_columns(feature_set1_merged_train_df, feature_set1_train_df, feature_set1_validate_df, feature_set1_test_df)\n",
    "feature_set2_merged_train_df, feature_set2_train_df, feature_set2_validate_df, feature_set2_test_df = scale_amount_columns(feature_set2_merged_train_df, feature_set2_train_df, feature_set2_validate_df, feature_set2_test_df)\n",
    "feature_set3_merged_train_df, feature_set3_train_df, feature_set3_validate_df, feature_set3_test_df = scale_amount_columns(feature_set3_merged_train_df, feature_set3_train_df, feature_set3_validate_df, feature_set3_test_df)\n",
    "feature_set4_merged_train_df, feature_set4_train_df, feature_set4_validate_df, feature_set4_test_df = scale_amount_columns(feature_set4_merged_train_df, feature_set4_train_df, feature_set4_validate_df, feature_set4_test_df)\n",
    "feature_set5_merged_train_df, feature_set5_train_df, feature_set5_validate_df, feature_set5_test_df = scale_amount_columns(feature_set5_merged_train_df, feature_set5_train_df, feature_set5_validate_df, feature_set5_test_df)\n",
    "feature_set6_merged_train_df, feature_set6_train_df, feature_set6_validate_df, feature_set6_test_df = scale_amount_columns(feature_set6_merged_train_df, feature_set6_train_df, feature_set6_validate_df, feature_set6_test_df)\n",
    "feature_set7_merged_train_df, feature_set7_train_df, feature_set7_validate_df, feature_set7_test_df = scale_amount_columns(feature_set7_merged_train_df, feature_set7_train_df, feature_set7_validate_df, feature_set7_test_df)\n",
    "feature_set8_merged_train_df, feature_set8_train_df, feature_set8_validate_df, feature_set8_test_df = scale_amount_columns(feature_set8_merged_train_df, feature_set8_train_df, feature_set8_validate_df, feature_set8_test_df)\n",
    "feature_set9_merged_train_df, feature_set9_train_df, feature_set9_validate_df, feature_set9_test_df = scale_amount_columns(feature_set9_merged_train_df, feature_set9_train_df, feature_set9_validate_df, feature_set9_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the min-max values are 0-1\n",
    "feature_set2_merged_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set9_merged_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold\n",
    "As after removing the rounds with 'House' position we end up with binary classification problem, we want to leave it like that. But in production environment we can bet Bull/Bear or pass the round without betting and wait for for the model to give confident prediction.\n",
    "We can get the probability for each class with sklearn function predict_proba. 0.5-0.5 means the model is completely not sure which class to choose, while 0.9-0.1 means its high probability that the record will be classified as the first class.\n",
    "Let's add another hyperparameter - threshold. The model will return a class label only when the probability will be higher than the threshold. Threshold can be in range of 0.5-1, however we have to determine it experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the scikit-learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def run(k, models_name, amount_data_df, x_train, y_train, x_test, y_test, clf, threshold=0.7, bet_size=1, min_multiplier=0):\n",
    "    # Check if the model already exists\n",
    "    model_name = f\"{models_name}_{str(clf)}_k{k}.pkl\"\n",
    "    model_dir = f\"../models/{model_name}\"\n",
    "\n",
    "    if os.path.exists(model_dir):\n",
    "        print(f\"Loading the model {model_dir}\")\n",
    "        with open(model_dir, 'rb') as f:\n",
    "            clf = pickle.load(f)\n",
    "    else:\n",
    "        print(f\"Training the model {clf}\")\n",
    "        clf.fit(x_train, y_train)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    total_accuracy = clf.score(x_test, y_test)\n",
    "\n",
    "    predicted_probability = clf.predict_proba(x_test)\n",
    "\n",
    "    # Create a df to compare the predicted_probability with the real y_test\n",
    "    df = pd.DataFrame(predicted_probability, columns=[\"Bear\", \"Bull\"], index=y_test.index)\n",
    "    df[\"position\"] = y_test.values\n",
    "    df[\"predicted_position\"] = np.where(df[\"Bear\"] > threshold, 0, np.nan)\n",
    "    df[\"predicted_position\"] = np.where(df[\"Bull\"] > threshold, 1, df[\"predicted_position\"])\n",
    "\n",
    "    df[\"correct\"] = np.where(df[\"position\"] == df[\"predicted_position\"], 1, 0)\n",
    "\n",
    "    df['bull_amount'] = amount_data_df['bull_amount']\n",
    "    df['bear_amount'] = amount_data_df['bear_amount']\n",
    "    df['total_amount'] = amount_data_df['total_amount']\n",
    "    \n",
    "    simulation_results_df = simulator.simulate(df[['position', 'bull_amount', 'bear_amount', 'total_amount']], df['predicted_position'], bet_size=bet_size, add_bet_to_pool = True, min_multiplier = min_multiplier)\n",
    "\n",
    "    # Drop rows where prediction is null and calculate the total profit\n",
    "    simulation_results_df = simulation_results_df[simulation_results_df['prediction'].notna()]\n",
    "    profit = simulation_results_df['profit'].sum()\n",
    "\n",
    "    played_rounds = (simulation_results_df['profit'] != 0).sum()\n",
    "    \n",
    "    # Get accuracy when probability is above threshold\n",
    "    accuracy = df[\"correct\"].sum() / (df['predicted_position'].count() + 0.0000001)  # Count only not NaN values\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    df['prediction_position_all'] = np.where(df[\"Bear\"] > df[\"Bull\"], 0, 1)\n",
    "    score = f1_score(df[\"position\"], df['prediction_position_all'], average=\"macro\")\n",
    "\n",
    "    # Save the model\n",
    "    if not os.path.exists(model_dir):\n",
    "        with open(model_dir,'wb') as f:\n",
    "            print(f\"Saving the model {model_dir}\")\n",
    "            pickle.dump(clf, f)\n",
    "\n",
    "    return {'classifier': str(clf), 'threshold': threshold, 'total_accuracy': total_accuracy, \"total_f1_score\": score,\n",
    "            'total_number_of_bets': len(y_test), 'accuracy_above_threshold': accuracy,\n",
    "            'number_of_bets_above_threshold': df['predicted_position'].count(), 'total_number_of_bets_placed': played_rounds, 'profit': profit}\n",
    "\n",
    "\n",
    "def main(models_name, amount_data_df, merged_train_df, clfs, threshold_list, cols_to_leave, bet_size=1, min_multiplier=0, k=10):\n",
    "    # k-fold cross-validation made for time series\n",
    "    tscv = TimeSeriesSplit(n_splits=k)\n",
    "    X, y = merged_train_df.drop([\"position\"], axis=1), merged_train_df[\"position\"]\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    for clf in clfs:\n",
    "        print(f'Classifier: {clf}')\n",
    "        for threshold in threshold_list:\n",
    "            print(f'Threshold: {threshold}')\n",
    "\n",
    "            kfold_data_list = []\n",
    "\n",
    "            current_k = 1\n",
    "            for train_index, test_index in tscv.split(X):\n",
    "                print(f'K: {current_k}')\n",
    "                print(f'indexes: train: {train_index} test:{test_index}')\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                \n",
    "                data = run(current_k, models_name, amount_data_df, X_train, y_train, X_test, y_test, clf, threshold, bet_size, min_multiplier)\n",
    "                kfold_data_list.append(data)\n",
    "\n",
    "                current_k += 1\n",
    "\n",
    "            # Calculate the mean values\n",
    "            mean_data = kfold_data_list[0]\n",
    "            mean_data['total_accuracy'] = sum(d['total_accuracy'] for d in kfold_data_list) / len(kfold_data_list)\n",
    "            mean_data['total_f1_score'] = sum(d['total_f1_score'] for d in kfold_data_list) / len(kfold_data_list)\n",
    "            mean_data['total_number_of_bets'] = sum(d['total_number_of_bets'] for d in kfold_data_list) / len(kfold_data_list)\n",
    "            mean_data['number_of_bets_above_threshold'] = sum(d['number_of_bets_above_threshold'] for d in kfold_data_list) / len(kfold_data_list)\n",
    "            mean_data['total_number_of_bets_placed'] = sum(d['total_number_of_bets_placed'] for d in kfold_data_list) / len(kfold_data_list)\n",
    "            mean_data['profit'] = sum(d['profit'] for d in kfold_data_list) / len(kfold_data_list)\n",
    "\n",
    "            if len([d['accuracy_above_threshold'] for d in kfold_data_list if not d['accuracy_above_threshold'] < 0.1]) == 0:\n",
    "                mean_data['accuracy_above_threshold'] = np.nan\n",
    "            else:\n",
    "                mean_data['accuracy_above_threshold'] = sum(d['accuracy_above_threshold'] for d in kfold_data_list if not d['accuracy_above_threshold'] < 0.1) / len([d['accuracy_above_threshold'] for d in kfold_data_list if not d['accuracy_above_threshold'] < 0.1])\n",
    "\n",
    "            \n",
    "            data_list.append(mean_data)\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Divide the classifiers into 2 groups (because of the training time)\n",
    "clfs = [RandomForestClassifier(100), RandomForestClassifier(500), RandomForestClassifier(1000),\n",
    "        DecisionTreeClassifier(criterion='entropy'), MLPClassifier(alpha=1, max_iter=1000), GaussianNB(),\n",
    "        KNeighborsClassifier(3), KNeighborsClassifier(5), KNeighborsClassifier(7),\n",
    "        KNeighborsClassifier(9), KNeighborsClassifier(11), LogisticRegression(max_iter=1000)]\n",
    "\n",
    "svc_clfs = [SVC(kernel='rbf', probability=True), SVC(kernel='linear', probability=True),\n",
    "            SVC(kernel='rbf', probability=True, C=10), SVC(kernel='linear', probability=True, C=10),\n",
    "            SVC(kernel='rbf', probability=True, C=50), SVC(kernel='linear', probability=True, C=50)]\n",
    "\n",
    "threshold_list = [0.6, 0.7, 0.8, 0.9, 0.95]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #1: total pool sizes for Bear/Bull bets (no additional info about the players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1_df = main(amount_data_df=merged_train_df, models_name='experiment1', merged_train_df=feature_set1_merged_train_df,\n",
    "                      clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set1, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment1_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment2_df = main(amount_data_df=merged_train_df, models_name='experiment2', merged_train_df=feature_set1_merged_train_df,\n",
    "                      clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set1, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment2_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #2: all 247 players bet info (Bull/Bear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment3_df = main(amount_data_df=merged_train_df, models_name='experiment3', merged_train_df=feature_set2_merged_train_df,\n",
    "                      clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set2, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment3_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment4_df = main(amount_data_df=merged_train_df, models_name='experiment4', merged_train_df=feature_set2_merged_train_df,\n",
    "                      clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set2, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment4_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #3: all 247 players bet info (Bull/Bear) + their bet size info (in CAKE tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment5_df = main(amount_data_df=merged_train_df, models_name='experiment5', merged_train_df=feature_set3_merged_train_df,\n",
    "                      clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set3, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This features set model takes much longer to train\n",
    "experiment5_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment6_df = main(amount_data_df=merged_train_df, models_name='experiment6', merged_train_df=feature_set3_merged_train_df,\n",
    "                      clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set3, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This features set model takes much longer to train\n",
    "experiment6_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #4: the best 10% of players bet info (Bull/Bear) according to their overall profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment7_df = main(amount_data_df=merged_train_df, models_name='experiment7', merged_train_df=feature_set4_merged_train_df,\n",
    "                      clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set4, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment7_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment8_df = main(amount_data_df=merged_train_df, models_name='experiment8', merged_train_df=feature_set4_merged_train_df,\n",
    "                      clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set4, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment8_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #5: the best 10% of players bet info (Bull/Bear) according to their overall profit + their bet size info (in CAKE tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment9_df = main(amount_data_df=merged_train_df, models_name='experiment9', merged_train_df=feature_set5_merged_train_df,\n",
    "                      clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set5, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment9_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment10_df = main(amount_data_df=merged_train_df, models_name='experiment10', merged_train_df=feature_set5_merged_train_df,\n",
    "                       clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set5, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment10_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #6: the best 20% of players bet info (Bull/Bear) according to their overall profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment11_df = main(amount_data_df=merged_train_df, models_name='experiment11', merged_train_df=feature_set6_merged_train_df,\n",
    "                       clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set6, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment11_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment12_df = main(amount_data_df=merged_train_df, models_name='experiment12', merged_train_df=feature_set6_merged_train_df,\n",
    "                       clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set6, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment12_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #7: the best 20% of players bet info (Bull/Bear) according to their overall profit + their bet size info (in CAKE tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment13_df = main(amount_data_df=merged_train_df, models_name='experiment13', merged_train_df=feature_set7_merged_train_df,\n",
    "                       clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set7, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment13_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment14_df = main(amount_data_df=merged_train_df, models_name='experiment14', merged_train_df=feature_set7_merged_train_df,\n",
    "                       clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set7, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment14_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #8: total amount of CAKE tokens bets for Bull/Bear of the best 10% of players + total number of bets (Bull/Bear) of the best 10% of players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment15_df = main(amount_data_df=merged_train_df, models_name='experiment15', merged_train_df=feature_set8_merged_train_df,\n",
    "                       clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set8, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment15_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment16_df = main(amount_data_df=merged_train_df, models_name='experiment16', merged_train_df=feature_set8_merged_train_df,\n",
    "                       clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set8, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment16_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature set #9: total amount of CAKE tokens bets for Bull/Bear of the best 20% of players + total number of bets (Bull/Bear) of the best 20% of players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment17_df = main(amount_data_df=merged_train_df, models_name='experiment17', merged_train_df=feature_set9_merged_train_df,\n",
    "                       clfs=clfs, threshold_list=threshold_list, cols_to_leave=feature_set9, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment17_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment18_df = main(amount_data_df=merged_train_df, models_name='experiment18', merged_train_df=feature_set9_merged_train_df,\n",
    "                       clfs=svc_clfs, threshold_list=threshold_list, cols_to_leave=feature_set9, bet_size=bet_size, min_multiplier=min_multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment18_df.sort_values(by='accuracy_above_threshold', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the accuracy_above_threshold and number_of_bets_above_threshold (and considering the speed of the models + the curse of dimensionality) from all the experiments, the models worth further analysing are SVC (feature set#8 and feature set #9).\n",
    "The hyperparameters should be tuned, to find the real profits that are possible to be made with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters\n",
    "I have selected 2 feature sets, the process of tuning the hyperparameters should be done separately for each of them. SVC can be tuned with:\n",
    "- C\n",
    "- gamma\n",
    "- kernel\n",
    "\n",
    "In addition we have to tune parameters of the simulator which are:\n",
    "- bet_size\n",
    "- min_multiplier\n",
    "- threshold\n",
    "\n",
    "I have to assume the pool sizes are +/- the same in all the rounds. In practice, during the bear market the pools are almost empty, and during the bull market people tend to bet more, which is better for us. That's why I have selected relatively short time periods for the data sets.\n",
    "\n",
    "There are 6 hyperparameters to tune in total for 2 different feature sets.\n",
    "To save some time I will use random search instead of grid search.\n",
    "\n",
    "Train_df will be used to train the models, validate_df to check the performance of the tuned models (hyperparameter tuning) and the final performance will be checked with an independent test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://otexts.com/fpp3/tscv.html\n",
    "- https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
